novela_texto <- gsub("([¿¡]) ", "\\1", novela_texto)
#novela_etiquetada <- gsub("…", "\\.\\.\\.", novela_etiquetada)
novela_texto <- gsub(" NA ", " ", novela_texto)
novela_texto <- tibble(texto = novela_texto)
novela_texto_tidy <- novela_texto %>%
unnest_tokens(oracion, texto, token = "sentences") %>%
mutate(num_pal = str_count(oracion, "\\w+"))
# Quita pasos intermedios
rm(novela_etiquetada, novela_texto)
re.analizado %>%
count(etiqueta, sort = T) %>%
filter(n > 100) %>%
mutate(etiqueta = reorder(etiqueta, n)) %>%
ggplot(aes(etiqueta, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
library(tidyverse)
library(tidytext)
# Previo
# Crea la tibble que servirá para limpiar por etiquetas
etiquetas_vacias <- tibble(etiqueta = c("PUNT", "PREP", "DET", "ART", "CCOP", "AUX", "CSUB", "PRON"))
View(etiquetas_vacias)
palabras_vacias <- read_tsv("~/Documents/GitHub/AnalisisTextual/vacias/vacias_esp.txt", quote="\"", col_names = T)
source("~/Documents/GitHub/AnalisisTextual/scripts/re.etiqueta.lo.R")
# Lee texto
ficheros <- list.files(path = "~/Desktop/Sucio")
entrada <- read_tsv(paste("~/Desktop/Sucio/", ficheros[3], sep=""), col_names = F) %>%
rename(palabra = X1, lema = X2, PoS = X3) %>% # renombra las columnas
add_row(palabra = NA, lema = NA, PoS = NA, .before = 1) %>% # Añade NA como primera fila
mutate (frase = as.factor(cumsum(is.na(palabra)))) %>% # Cuenta y numera oraciones
drop_na() # Borra los NA
View(entrada)
View(re.etiqueta.lo)
View(entrada)
# Invoca la función re.etiqueta.lo
re.analizado <- re.etiqueta.lo(entrada)
View(re.analizado)
View(entrada)
View(re.etiqueta.lo)
source("~/Documents/GitHub/AnalisisTextual/scripts/re.etiqueta.lo.R")
# Lee texto
ficheros <- list.files(path = "~/Desktop/Sucio")
entrada <- read_tsv(paste("~/Desktop/Sucio/", ficheros[3], sep=""), col_names = F) %>%
rename(palabra = X1, lema = X2, PoS = X3) %>% # renombra las columnas
add_row(palabra = NA, lema = NA, PoS = NA, .before = 1) %>% # Añade NA como primera fila
mutate (frase = as.factor(cumsum(is.na(palabra)))) %>% # Cuenta y numera oraciones
drop_na() # Borra los NA
# Invoca la función re.etiqueta.lo
re.analizado <- re.etiqueta.lo(entrada)
View(re.analizado)
re.analizado <- within(re.analizado, etiqueta == "PUNCT" & palabra == "." <- ".")
re.analizado <- within(re.analizado, etiqueta [etiqueta == "PUNT" & palabra == "."] <- ".")
library(tidyverse)
library(tidytext)
# Previo
# Crea la tibble que servirá para limpiar por etiquetas
etiquetas_vacias <- tibble(etiqueta = c("PUNT", "PREP", "DET", "ART", "CCOP", "AUX", "CSUB", "PRON"))
palabras_vacias <- read_tsv("~/Documents/GitHub/AnalisisTextual/vacias/vacias_esp.txt", quote="\"", col_names = T)
source("~/Documents/GitHub/AnalisisTextual/scripts/re.etiqueta.lo.R")
View(re.etiqueta.lo)
source("~/Documents/GitHub/AnalisisTextual/scripts/re.etiqueta.lo.R")
# Lee texto
ficheros <- list.files(path = "~/Desktop/Sucio")
entrada <- read_tsv(paste("~/Desktop/Sucio/", ficheros[3], sep=""), col_names = F) %>%
rename(palabra = X1, lema = X2, PoS = X3) %>% # renombra las columnas
add_row(palabra = NA, lema = NA, PoS = NA, .before = 1) %>% # Añade NA como primera fila
mutate (frase = as.factor(cumsum(is.na(palabra)))) %>% # Cuenta y numera oraciones
drop_na() # Borra los NA
View(entrada)
# Invoca la función re.etiqueta.lo
re.analizado <- re.etiqueta.lo(entrada)
View(re.analizado)
re.analizado %>%
count(palabra == str_detect(palabra, "^\\w+mente"))
re.analizado %>%
count(palabra == str_detect(palabra, "^\w+mente"))
re.analizado %>%
count(palabra == str_detect(palabra, "\\w+mente"))
re.analizado %>%
count(re.analizado$palabra == str_detect(re.analizado$palabra, "\\w+mente"))
str_detect(re.analizado$palabra, "\\w+mente")
re.analizado %>%
count(re.analizado$palabra != str_detect(re.analizado$palabra, "\\w+mente"))
re.analizado %>%
filter(re.analizado$palabra == str_detect(re.analizado$palabra, "\\w+mente"))
re.analizado %>%
filter(re.analizado$palabra == str_detect(re.analizado$palabra, "\\w+mente"))
View(re.analizado)
ad_mente <- re.analizado %>%
filter(re.analizado$palabra == str_detect(re.analizado$palabra, "\\w+mente"))
View(ad_mente)
ad_mente <- re.analizado %>%
filter(str_detect(re.analizado$palabra, "\\w+mente"))
View(ad_mente)
ad_mente <- re.analizado %>%
filter(str_detect(re.analizado$palabra, "^\\w+mente$"))
View(ad_mente)
# Borra puntuación
sin_puntuacion <- re.analizado %>%
filter(etiqueta != "PUNT")
View(sin_puntuacion)
re.analizado %>%
filter(str_detect(re.analizado$palabra, "^\\w+mente$") & etiqueta !="RG")
re.analizado %>%
filter(str_detect(re.analizado$palabra, "^\\w+mente$"), etiqueta != "RG")
re.analizado %>%
filter(str_detect(re.analizado$palabra, "^\\w+mente$") & etiqueta = "RG")
re.analizado %>%
filter(str_detect(re.analizado$palabra, "^\\w+mente$"), etiqueta != "ADV")
library(tidyverse)
library(tidytext)
library(scales)
library(igraph)
library(ggraph)
library(widyr)
library(psych)
library(kableExtra)
install.packages("kableExtra")
library(knitr)
library(plotly)
install.packages("plotly")
install.packages("plotly")
install.packages(c("ggcorrplot", "reticulate", "packcircles", "patchwork"))
raw_data <- read.csv("~/Desktop/work/the-office-lines-scripts.txt", header = T, sep = "\t", stringsAsFactors = F)
View(raw_data)
raw_data
library(tidyverse)
library(tidytext)
library(scales)
library(googlesheets)
library(igraph)
library(ggraph)
library(widyr)
library(psych)
library(kableExtra)
library(knitr)
library(plotly)
library(y)
library(reticulate)
library(cleanNLP)
library(packcircles)
library(patchwork)
raw_data <- read.csv("~/Desktop/work/the-office-lines-scripts.txt", header = T, sep = "\t", stringsAsFactors = F)
raw_data <- as_tibble(raw_data)
raw_data
mod_data <- raw_data %>%
filter(deleted == "FALSE") %>%
mutate(actions = str_extract_all(line_text, "\\[.*?\\]"),
line_text_mod = str_trim(str_replace_all(line_text, "\\[.*?\\]", ""))) %>%
mutate_at(vars(line_text_mod), funs(str_replace_all(., "���","'"))) %>%
mutate_at(vars(speaker), funs(tolower)) %>%
mutate_at(vars(speaker), funs(str_trim(str_replace_all(., "\\[.*?\\]", "")))) %>%
mutate_at(vars(speaker), funs(str_replace_all(., "micheal|michel|michae$", "michael")))
View(mod_data)
total_episodes <- mod_data %>%
unite(season_ep, season, episode, remove = FALSE) %>%
summarise(num_episodes = n_distinct(season_ep)) %>%
as.integer()
episode_proportion <- mod_data %>%
unite(season_ep, season, episode, remove = FALSE) %>%
group_by(speaker) %>%
summarise(num_episodes = n_distinct(season_ep)) %>%
mutate(proportion = round((num_episodes / total_episodes) * 100, 1)) %>%
arrange(desc(num_episodes))
View(episode_proportion)
total_scenes <- mod_data %>%
unite(season_ep_scene, season, episode, scene, remove = FALSE) %>%
summarise(num_scenes = n_distinct(season_ep_scene)) %>%
as.integer()
# proportion of scenes each character was in
scene_proportion <- mod_data %>%
unite(season_ep_scene, season, episode, scene, remove = FALSE) %>%
group_by(speaker) %>%
summarise(num_scenes = n_distinct(season_ep_scene)) %>%
mutate(proportion = round((num_scenes / total_scenes) * 100, 1)) %>%
arrange(desc(num_scenes))
line_proportion <- mod_data %>%
count(speaker) %>%
mutate(proportion = round((n / sum(n)) * 100, 1)) %>%
arrange(desc(n))
View(line_proportion)
# define main characters based on line proportion
main_characters <- factor(line_proportion %>%
filter(proportion >= 1) %>%
pull(speaker) %>%
fct_inorder()
)
line_proportion_by_season <- mod_data %>%
group_by(season) %>%
count(speaker) %>%
mutate(proportion = round((n / sum(n)) * 100, 1)) %>%
arrange(season, desc(proportion))
line_proportion_over_time <- line_proportion_by_season %>%
filter(speaker %in% main_characters) %>%
ggplot(aes(x = season, y = proportion, color = speaker, label = proportion)) +
geom_point(size = 2) +
geom_line() +
scale_x_continuous(breaks = seq(1, 9, 1)) +
theme_minimal() +
theme(legend.position = "none") +
labs(y = "% of lines",
title = "% of Lines by Season") +
theme(plot.title = element_text(hjust = 0.5)) +
facet_wrap(~ factor(str_to_title(speaker), levels = str_to_title(main_characters)), ncol = 3) +
geom_text(vjust = -1.2, size = 3.5) +
ylim(0, 50) +
scale_color_manual(values = office_colors)
line_proportion_over_time
line_proportion_over_time <- line_proportion_by_season %>%
filter(speaker %in% main_characters) %>%
ggplot(aes(x = season, y = proportion, color = speaker, label = proportion)) +
geom_point(size = 2) +
geom_line() +
scale_x_continuous(breaks = seq(1, 9, 1)) +
theme_minimal() +
theme(legend.position = "none") +
labs(y = "% of lines",
title = "% of Lines by Season") +
theme(plot.title = element_text(hjust = 0.5)) +
facet_wrap(~ factor(str_to_title(speaker), levels = str_to_title(main_characters)), ncol = 3) +
geom_text(vjust = -1.2, size = 3.5) +
ylim(0, 50)# +
line_proportion_over_time
tidy_tokens <- mod_data %>%
select(line = id, line_text_mod, everything(), -line_text, -actions, -deleted) %>%
unnest_tokens(word, line_text_mod, strip_numeric = TRUE) %>%
mutate_at(vars(word), funs(str_replace_all(., "'s$", "")))
install.packages("tidytext")
install.packages("tidytext")
library(tidyverse)
library(tidytext)
library(scales)
library(googlesheets)
library(igraph)
#library(googlesheets)
library(igraph)
library(ggraph)
library(widyr)
library(psych)
library(kableExtra)
library(knitr)
library(plotly)
library(ggcorrplot)
library(reticulate)
library(cleanNLP)
install.packages("cleanNLP")
library(packcircles)
library(patchwork)
tidy_tokens <- mod_data %>%
select(line = id, line_text_mod, everything(), -line_text, -actions, -deleted) %>%
unnest_tokens(word, line_text_mod, strip_numeric = TRUE) %>%
mutate_at(vars(word), funs(str_replace_all(., "'s$", "")))
tidy_tokens_no_stop <- tidy_tokens %>%
anti_join(stop_words, by = "word")
library(tidyverse)
library(tidytext)
library(scales)
#library(googlesheets)
library(igraph)
library(ggraph)
library(widyr)
library(psych)
library(kableExtra)
library(knitr)
library(plotly)
library(ggcorrplot)
library(reticulate)
library(cleanNLP)
library(packcircles)
library(patchwork)
raw_data <- read.csv("~/Desktop/work/the-office-lines-scripts.txt", header = T, sep = "\t", stringsAsFactors = F)
raw_data <- as_tibble(raw_data)
mod_data <- raw_data %>%
filter(deleted == "FALSE") %>%
mutate(actions = str_extract_all(line_text, "\\[.*?\\]"),
line_text_mod = str_trim(str_replace_all(line_text, "\\[.*?\\]", ""))) %>%
mutate_at(vars(line_text_mod), funs(str_replace_all(., "���","'"))) %>%
mutate_at(vars(speaker), funs(tolower)) %>%
mutate_at(vars(speaker), funs(str_trim(str_replace_all(., "\\[.*?\\]", "")))) %>%
mutate_at(vars(speaker), funs(str_replace_all(., "micheal|michel|michae$", "michael")))
total_episodes <- mod_data %>%
unite(season_ep, season, episode, remove = FALSE) %>%
summarise(num_episodes = n_distinct(season_ep)) %>%
as.integer()
episode_proportion <- mod_data %>%
unite(season_ep, season, episode, remove = FALSE) %>%
group_by(speaker) %>%
summarise(num_episodes = n_distinct(season_ep)) %>%
mutate(proportion = round((num_episodes / total_episodes) * 100, 1)) %>%
arrange(desc(num_episodes))
total_scenes <- mod_data %>%
unite(season_ep_scene, season, episode, scene, remove = FALSE) %>%
summarise(num_scenes = n_distinct(season_ep_scene)) %>%
as.integer()
# proportion of scenes each character was in
scene_proportion <- mod_data %>%
unite(season_ep_scene, season, episode, scene, remove = FALSE) %>%
group_by(speaker) %>%
summarise(num_scenes = n_distinct(season_ep_scene)) %>%
mutate(proportion = round((num_scenes / total_scenes) * 100, 1)) %>%
arrange(desc(num_scenes))
line_proportion <- mod_data %>%
count(speaker) %>%
mutate(proportion = round((n / sum(n)) * 100, 1)) %>%
arrange(desc(n))
# define main characters based on line proportion
main_characters <- factor(line_proportion %>%
filter(proportion >= 1) %>%
pull(speaker) %>%
fct_inorder()
)
line_proportion_by_season <- mod_data %>%
group_by(season) %>%
count(speaker) %>%
mutate(proportion = round((n / sum(n)) * 100, 1)) %>%
arrange(season, desc(proportion))
line_proportion_over_time <- line_proportion_by_season %>%
filter(speaker %in% main_characters) %>%
ggplot(aes(x = season, y = proportion, color = speaker, label = proportion)) +
geom_point(size = 2) +
geom_line() +
scale_x_continuous(breaks = seq(1, 9, 1)) +
theme_minimal() +
theme(legend.position = "none") +
labs(y = "% of lines",
title = "% of Lines by Season") +
theme(plot.title = element_text(hjust = 0.5)) +
facet_wrap(~ factor(str_to_title(speaker), levels = str_to_title(main_characters)), ncol = 3) +
geom_text(vjust = -1.2, size = 3.5) +
ylim(0, 50)# +
line_proportion_over_time
tidy_tokens <- mod_data %>%
select(line = id, line_text_mod, everything(), -line_text, -actions, -deleted) %>%
unnest_tokens(word, line_text_mod, strip_numeric = TRUE) %>%
mutate_at(vars(word), funs(str_replace_all(., "'s$", "")))
tidy_tokens <- mod_data %>%
select(line = id, line_text_mod, everything(), -line_text, -actions, -deleted) %>%
unnest_tokens(word, line_text_mod) %>%
mutate_at(vars(word), funs(str_replace_all(., "'s$", "")))
tidy_tokens_no_stop <- tidy_tokens %>%
anti_join(stop_words, by = "word")
custom_stop_words <- bind_rows(data_frame(word = c("yeah", "hey", "uh", "um", "huh", "hmm", "ah", "umm", "uhh", "gonna", "na", "ha", "gotta"),
lexicon = c("custom")),
stop_words)
View(custom_stop_words)
library(tidyverse)
library(tidytext)
library(scales)
library(igraph)
library(ggraph)
options(scipen = 999) # que no emplee notación científica
# Carga un paquete de datos que contiene las stop-words españolas
vacias_cast <- as_tibble(read_lines("~/Documents/Github/AnalisisTextual/vacias/vacias_cast.txt")) %>%
rename(palabra = value)
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
View(partidas)
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
partidas[914]
partidas[,914]
partidas[914,]
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
View(partidas)
View(partidas)
partidas[texto,914]
partidas[914, texto]
partidas[914, 5]
a <- partidas[914, 5]
a
rm(a)
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
partidas[1107, texto]
partidas[1107,]
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
partidas[1107,]
partidas[1108,]
partidas[1106,]
partidas
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
partidas[1134,]
partidas[1334,]
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
partidas[1352,]
partidas[1352,5]
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
partidas[2304,5]
partidas[2304,]
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
partidas[2418,]
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
partidas[2427,]
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
partidas[2459,]
partidas[2459,5]
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
partidas[2459,5]
partidas[2459,]
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
partidas[2621,]
partidas[2621,5]
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
library(tidyverse)
library(tidytext)
library(scales)
library(igraph)
library(ggraph)
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
View(partidas)
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
View(partidas)
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
View(partidas)
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
# Carga el texto
partidas <- read_delim("~/Desktop/WORK/SP_LOP_tabla.txt", delim = "\t")
View(partidas)
install.packages("epubr")
library(tidyverse)
library(tidytext)
library(rvest)
theme_set(theme_light())
url <- "https://www.nytimes.com/2018/09/05/opinion/trump-white-house-anonymous-resistance.html"
# tail(-1) removes the first paragraph, which is an editorial header
op_ed <- read_html(url) %>%
html_nodes(".e2kc3sl0") %>%
html_text() %>%
tail(-1) %>%
data_frame(text = .)
View(op_ed)
url <- "https://elpais.com/elpais/2018/08/30/opinion/1535628781_880162.html"
# tail(-1) removes the first paragraph, which is an editorial header
op_ed <- read_html(url) %>%
html_nodes(xpath = "//p") %>%
html_text() %>%
#tail(-1) %>%
data_frame(text = .)
View(op_ed)
library(tidyverse)
library(tidytext)
library(rvest)
theme_set(theme_light())
url <- "~/Documents/GitHub/XML-TEI/textos/SP-LOP.xml"
op_ed <- read_html(url) %>%
html_nodes(xpath = "//div") %>%
html_text() %>%
data_frame(text = .)
op_ed$text <- gsub("-\n\t\t", "", op_ed$text, perl = T)
op_ed$text <- gsub("\n\t\t", " ", op_ed$text, perl = T)
op_ed$text <- gsub("\n", "", op_ed$text, perl = T)
write_tsv("op_ed", "~/Desktop/raro.txt")
View(op_ed)
write_lines("op_ed", "~/Desktop/raro.txt")
write_lines(op_ed, "~/Desktop/raro.txt")
library(tidyverse)
tbl <- list.files(pattern = "*.txt") %>%
map_chr(~ read_file(.)) %>%
data_frame(texto = .) %>%
mutate(filename = basename(.x))
tbl <- list.files(pattern = "*.txt") %>%
map_chr(~ read_file(.)) %>%
data_frame(texto = .) %>%
mutate(filename = basename(.))
tbl <- list.files(pattern = "*.txt") %>%
map_chr(~ read_file(.)) %>%
data_frame(texto = .) %>%
mutate(filename = basename())
setwd("~/Documents/GitHub/AnalisisTextual/mensajes/corpus")
tbl <- list.files(pattern = "*.txt") %>%
map_chr(~ read_file(.)) %>%
data_frame(texto = .) %>%
mutate(filename = basename())
tbl <- list.files(pattern = "*.txt") %>%
map_chr(~ read_file(.)) %>%
data_frame(texto = .) %>%
mutate(filename = basename(path = "~/Documents/GitHub/AnalisisTextual/mensajes/corpus"))
tbl <- list.files(pattern = "*.txt") %>%
map_chr(~ read_file(.)) %>%
data_frame(texto = .) %>%
mutate(filename = basename(path = "~/Documents/GitHub/AnalisisTextual/mensajes/corpus/"))
tbl <- list.files(pattern = "*.txt") %>%
map_chr(~ read_file(.)) %>%
data_frame(texto = .) %>%
mutate(anno = basename(path = "~/Documents/GitHub/AnalisisTextual/mensajes/corpus/"))
datos <- read.csv("~/Documents/GitHub/AnalisisTextual/mensajes/duracion_mensajes.txt/", header = T, sep = "\t")
datos <- read.csv("~/Documents/GitHub/AnalisisTextual/mensajes/duracion_mensajes.txt", header = T, sep = "\t")
datos <- read.csv("~/Documents/GitHub/AnalisisTextual/mensajes/duracion_mensajes.txt", header = T, sep = "\t", col_types = list(col_integer(), col_factor(), col_character()))
