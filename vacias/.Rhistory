alatriste01 <- readLines("~/Dropbox/_R-nuevo/_TEXTOS/-alatriste/txt/Alatriste-1_El-capitan-Alatriste_Arturo-Perez-Reverte.txt")
alatriste01_unido <- paste(alatriste01, collapse = " ")
alatriste01_tokens <- unlist(strsplit(alatriste01_unido, " ")
alatriste01_tokens <- unlist(strsplit(alatriste01_unido, " ")
)
alatriste01_tokens <- unlist(strsplit(alatriste01_unido, " "))
d <- rpois(73,5)
d
x <- seq_along(alatriste01_tokens)
x <- as.character(seq_along(alatriste01_tokens))
x <- seq_along(alatriste01_tokens)
alatriste01_segmentos <- split(alatriste01_segmentos, ceiling(x/max))
alatriste01_segmentos <- split(alatriste01_tokens, ceiling(x/max))
length(alatriste01_tokens)
alatriste01_segmentos <- split(alatriste01_tokens, ceiling(length(alatriste01_tokens)/500))
alatriste01_segmentos <- unlist(split(alatriste01_tokens, ceiling(length(alatriste01_tokens)/500)))
alatriste01_unido <- paste(alatriste01, collapse = " ")
alatriste01_tokens <- unlist(strsplit(alatriste01_unido, " "))
alatriste01 <- readLines("~/Dropbox/_R-nuevo/_TEXTOS/-alatriste/txt/Alatriste-1_El-capitan-Alatriste_Arturo-Perez-Reverte.txt")
alatriste01_unido <- paste(alatriste01, collapse = " ")
alatriste01_tokens <- unlist(strsplit(alatriste01_unido, " "))
a <- which(alatriste01_unido ==" ")
which(alatriste01_unido == " ")
alatriste01_tokens <- unlist(strsplit(alatriste01_unido, " "))
length(alatriste01_tokens)/500
alatriste01 <- readLines("~/Dropbox/_R-nuevo/_TEXTOS/-alatriste/txt/Alatriste-1_El-capitan-Alatriste_Arturo-Perez-Reverte.txt")
alatriste01_unido <- paste(alatriste01, collapse = " ")
alatriste01_tokens <- strsplit(alatriste01_unido, " ")
alatriste01_tokens <- unlist(alatriste01_tokens)
alatriste01 <- readLines("~/Dropbox/_R-nuevo/_TEXTOS/-alatriste/txt/Alatriste-1_El-capitan-Alatriste_Arturo-Perez-Reverte.txt")
alatriste01_unido <- paste(alatriste01, collapse = " ")
alatriste01_tokens <- strsplit(alatriste01_unido, " ")
alatriste01_tokens <- unlist(alatriste01_tokens)
long_maxima <- length(alatriste01_tokens)/400 # Viene a ser el número de palabras en una página
x <- seq_along(alatriste01_tokens)
alatriste01_paginas <- split(alatriste01_tokens, ceiling(x/long_maxima))
alatriste01_paginas
unlist(alatriste01_paginas)
a <- as.data.frame(alatriste01_paginas)
unir <- function(x){
paste(x, collapse = " ")
}
alatriste01_paginas <- lapply(alatriste01_paginas, unir)
paginas <- unlist(alatriste01_paginas)
paginas[1:2]
str(paginas)
paginas <- as.character(paginas)
alatriste01 <- readLines("~/Dropbox/_R-nuevo/_TEXTOS/-alatriste/txt/Alatriste-1_El-capitan-Alatriste_Arturo-Perez-Reverte.txt")
alatriste01_unido <- paste(alatriste01, collapse = " ")
alatriste01_tokens <- strsplit(alatriste01_unido, " ")
alatriste01_tokens <- unlist(alatriste01_tokens)
long_maxima <- length(alatriste01_tokens)/400 # Viene a ser el número de palabras en una página
x <- seq_along(alatriste01_tokens)
alatriste01_paginas <- split(alatriste01_tokens, ceiling(x/long_maxima))
alatriste01_paginas
alatriste01_paginas <- lapply(alatriste01_paginas, function(x) paste(x, collapse = " "))
texto_entrada <- readLines("~/Dropbox/_R-nuevo/_TEXTOS/-alatriste/txt/Alatriste-1_El-capitan-Alatriste_Arturo-Perez-Reverte.txt")
texto_unido <- paste(texto_entrada, collapse = " ")
texto_palabras <- unlist(strsplit(texto_unido, " "))
long_maxima <- length(texto_palabras)/400 # Viene a ser el número de palabras en una página
x <- seq_along(texto_palabras)
texto_paginas <- split(texto_palabras, ceiling(x/long_maxima))
texto_paginas <- as-character(unlist(lapply(texto_paginas, function(x) paste(x, collapse = " ")))))
texto_paginas <- as-character(unlist(lapply(texto_paginas, function(x) paste(x, collapse = " "))))
texto_paginas <- split(texto_palabras, ceiling(x/long_maxima))
texto_paginas <- lapply(texto_paginas, function(x) paste(x, collapse = " "))
texto_paginas <- lapply(texto_paginas, function(x) paste(x, collapse = " "))
texto_paginas <- as.character(unlist(texto_paginas))
library(stylo)
stylo()
documento <- paste("Usar el análisis textual automatizado es sencillo",
"En este curso te enseñares cómo hacerlo",
"Partirás de cero y no sabemos dónde llegarás")
df <- data.frame(texto = documento)
library(tidyverse)
library(tidyverse) # Librería que carga todas la librería necesarias para el sistema tidy
library(tidytext) # Librería específica para manejar textos.
documento <- paste("Usar el análisis textual automatizado es sencillo.",
"En este curso te enseñaremos cómo hacerlo.",
"Partirás de cero y no sabemos dónde llegarás.")
df <- data.frame(texto = documento)
# Lo primero que vamos hacer ahora es dividir estas tres sencillas oraciones en oraciones…
documento_lineas <- unnest_tokens(df, texto, linea, token = "sentences", to_lower = F)
# Lo primero que vamos hacer ahora es dividir estas tres sencillas oraciones en oraciones…
documento_lineas <- unnest_tokens(df, input = texto, output = linea, token = "sentences", to_lower = F)
# Lo primero que vamos hacer ahora es dividir estas tres sencillas oraciones en oraciones…
documento_lineas <- unnest_tokens(df, input = texto, output = oración, token = "sentences", to_lower = F)
document_lineas$NumOracion <- seq_along(documento_lineas$oración)
documento_lineas$NumOracion <- seq_along(documento_lineas$oración)
View(documento_lineas)
documento_lineas
# Ahora lo vamos a dividir en palabras…
documento_palabras <- documento_lineas %>%
unnest_tokens(output = palabra, input = línea, token = "words")
# Ahora lo vamos a dividir en palabras…
documento_palabras <- documento_lineas %>%
unnest_tokens(output = palabra, input = oración, token = "words")
documento_palabras
# Un documento muy tonto…
documento <- paste("Usar el análisis textual automatizado es sencillo.",
"En este curso te enseñaremos cómo hacerlo.",
"Partirás de cero y no sabemos dónde llegarás.")
df <- data_frame(texto = documento)
# Lo primero que vamos hacer ahora es dividir estas tres sencillas oraciones en oraciones…
documento_lineas <- unnest_tokens(df, input = texto, output = oración, token = "sentences", to_lower = F)
documento_lineas$NumOracion <- seq_along(documento_lineas$oración)
documento_lineas
# Ahora lo vamos a dividir en palabras…
documento_palabras <- documento_lineas %>%
unnest_tokens(output = palabra, input = oración, token = "words")
documento_palabras
# pero también podemos dividirlo en bigramas, es decir, en secuencias de dos palabras en dos…
documento_bigramas <- documento_lineas %>%
unnest_tokens(output = bigrama, input = oración, token = "ngrams", n = 2)
documento_bigramas
# pero también podemos dividirlo en bigramas, es decir, en secuencias de tres palabras en tres…
documento_trigramas <- documento_lineas %>%
unnest_tokens(output = trigrama, input = oración, token = "ngrams", n = 3)
documento_trigramas
# Si miras cuántas palabras hay
documento_palabras %>%
(count)
# Si miras cuántas palabras hay
documento_palabras %>%
(count, sort = T)
# Si miras cuántas palabras hay
documento_palabras %>%
(count, sort = T)
# Si miras cuántas palabras hay
documento_palabras %>%
(count, order() = T)
# Si miras cuántas palabras hay
documento_palabras %>%
(count sort = T)
# Si miras cuántas palabras hay
documento_palabras %>%
count(sort = T)
# Si miras cuántas palabras hay
documento_palabras %>%
count(documento_palabras, sort = T)
# Si miras cuántas palabras hay
documento_palabras %>%
count(palabras, sort = T)
# Si miras cuántas palabras hay
documento_palabras %>%
count(palabra, sort = T)
# Si miras cuántas palabras hay
documento_palabras %>%
count(palabra)
View(documento_palabras)
# Si miras cuántas palabras hay
documento_palabras %>%
count(palabra)
load("~/Dropbox/_R-nuevo/_SENTIMIENTO_ARTICULO/LEXICONES/master/datos_esp(original).rda")
setwd("~/Dropbox/WORK/AnalisisTextual/vacias")
save(vacias_esp, file="vacias_esp.rda")
# El paquete tidytext incluye un listado de palabras vacías, es decir, de palabras de alta frecuencia
# de aparición pero escaso o nulo contenido. En muchos casos esas palabras no sirven para nada, porque
# nada nos pueden decir sobre el contenido del texto, por lo que a veces en necesario eliminarlas.
# Como decíamos, tidytext trae de serie una lista de stopwords (palabras vacías), pero solo para el
# ingles. No hay problema. Hemos creado un fichero que se puede cargar directamente
load("https://github.com/7PartidasDigital/AnalisisTextual/blob/master/vacias/vacias_esp.rda")
load("~/Dropbox/WORK/AnalisisTextual/vacias/vacias_esp.rda")
write.csv(vacias:esp, file = "vacias.esp", sep = "\t", row.names = F, col.names = T)
write.csv(vacias_esp, file = "vacias.esp", sep = "\t", row.names = F, col.names = T)
vacias_esp <- read_csv("vacias.esp")
vacias_esp <- as.tibble(read_csv("vacias.esp"))
vacias_esp <- as_tibble(read_csv("vacias.esp"))
vacias_esp
# El paquete tidytext incluye un listado de palabras vacías, es decir, de palabras de alta frecuencia
# de aparición pero escaso o nulo contenido. En muchos casos esas palabras no sirven para nada, porque
# nada nos pueden decir sobre el contenido del texto, por lo que a veces en necesario eliminarlas.
# Como decíamos, tidytext trae de serie una lista de stopwords (palabras vacías), pero solo para el
# ingles. No hay problema. Hemos creado un fichero que se puede cargar directamente
vacias_esp <- read_csv("vacias_esp.txt")
vacias_esp
# El paquete tidytext incluye un listado de palabras vacías, es decir, de palabras de alta frecuencia
# de aparición pero escaso o nulo contenido. En muchos casos esas palabras no sirven para nada, porque
# nada nos pueden decir sobre el contenido del texto, por lo que a veces en necesario eliminarlas.
# Como decíamos, tidytext trae de serie una lista de stopwords (palabras vacías), pero solo para el
# ingles. No hay problema. Hemos creado un fichero que se puede cargar directamente
vacias_esp <- as_tibble(read_csv("vacias_esp.txt"))
vacias_esp <-
vacias_esp
# Un documento muy tonto…
documento <- paste("Usar el análisis textual automatizado es sencillo.",
"En este curso te enseñaremos cómo hacerlo.",
"Partirás de cero y no sabemos dónde llegarás.")
df <- data_frame(texto = documento)
# Lo primero que vamos hacer ahora es dividir estas tres sencillas oraciones en oraciones…
documento_lineas <- unnest_tokens(df, input = texto, output = oración, token = "sentences", to_lower = F)
documento_lineas$NumOracion <- seq_along(documento_lineas$oración)
documento_lineas
# Ahora lo vamos a dividir en palabras…
documento_palabras <- documento_lineas %>%
unnest_tokens(output = palabra, input = oración, token = "words")
documento_palabras
# pero también podemos dividirlo en bigramas, es decir, en secuencias de dos palabras en dos…
documento_bigramas <- documento_lineas %>%
unnest_tokens(output = bigrama, input = oración, token = "ngrams", n = 2)
documento_bigramas
# pero también podemos dividirlo en bigramas, es decir, en secuencias de tres palabras en tres…
documento_trigramas <- documento_lineas %>%
unnest_tokens(output = trigrama, input = oración, token = "ngrams", n = 3)
documento_trigramas
# El paquete tidytext incluye un listado de palabras vacías, es decir, de palabras de alta frecuencia
# de aparición pero escaso o nulo contenido. En muchos casos esas palabras no sirven para nada, porque
# nada nos pueden decir sobre el contenido del texto, por lo que a veces en necesario eliminarlas.
# Como decíamos, tidytext trae de serie una lista de stopwords (palabras vacías), pero solo para el
# ingles. No hay problema. Hemos creado un fichero que se puede cargar directamente
vacias_esp <- read_csv("https://raw.githubusercontent.com/7PartidasDigital/AnalisisTextual/master/vacias/vacias_esp.txt.txt")
# El paquete tidytext incluye un listado de palabras vacías, es decir, de palabras de alta frecuencia
# de aparición pero escaso o nulo contenido. En muchos casos esas palabras no sirven para nada, porque
# nada nos pueden decir sobre el contenido del texto, por lo que a veces en necesario eliminarlas.
# Como decíamos, tidytext trae de serie una lista de stopwords (palabras vacías), pero solo para el
# ingles. No hay problema. Hemos creado un fichero que se puede cargar directamente
vacias_esp <- read_csv("https://raw.githubusercontent.com/7PartidasDigital/AnalisisTextual/master/vacias/vacias_esp.txt")
# Pero el texto que acabamos de ver es poco práctico. Vamos a cargar un artículo de Mariano José Larra
texto <- read_lines("https://github.com/7PartidasDigital/AnalisisTextual/blob/master/textos/larra_caza.txt", skip = 5)
# Pero el texto que acabamos de ver es poco práctico. Vamos a cargar un artículo de Mariano José Larra
texto <- read_lines("https://raw.githubusercontent.com/7PartidasDigital/AnalisisTextual/master/textos/larra_caza.txt", skip = 5)
texto
# Ya tienes el texto. Lo que has bajado no tiene el formato adecuado para trabajar dentro del ecosistema tidy.
texto_tidy <- data_frame(texto = texto)
View(texto_tidy)
# Pero el texto que acabamos de ver es poco práctico. Vamos a cargar un artículo de Mariano José Larra
texto <- read_lines("https://raw.githubusercontent.com/7PartidasDigital/AnalisisTextual/master/textos/larra_caza.txt", skip = 5)
# Ya tienes el texto. Lo que has bajado no tiene el formato adecuado para trabajar dentro del ecosistema tidy.
texto_tidy <- data_frame(texto = texto)
View(texto_tidy)
# Ahora lo tienes convertido en una tabla (se llama tibble) de una columna en la que cada párrafo está en una fila
texto_tidy
# Ahora ya podemos empezar a hacerle algunas diabluras…
# Lo primero lo vamos a dividir en oraciones…
texto_oraciones <- unnest_tokens(df, texto, oración, token = "sentences", to_lower = F)
# Ahora ya podemos empezar a hacerle algunas diabluras…
# Lo primero lo vamos a dividir en oraciones…
texto_oraciones <- unnest_tokens(df, texto, output = oración, token = "sentences", to_lower = F)
texto_oraciones
# Ahora ya podemos empezar a hacerle algunas diabluras…
# Lo primero lo vamos a dividir en oraciones…
texto_oraciones <- unnest_tokens(texto_tidy, texto, output = oración, token = "sentences", to_lower = F)
texto_oraciones
# Ahora las vamos ha identificar con el número de oración
texto_lineas$NumOracion <- seq_along(texto_lineas$oración)
# Ahora las vamos ha identificar con el número de oración
texto_oraciones$NumOracion <- seq_along(texto_oraciones$oración)
documento_oraciones
texto_oraciones
# Quizá te interesara numerar también los párrafos, para que así pudieras localizar las cosas con mayor facilidad
texto_tidy$NumParr <- seq_along(texto_tidy)
texto_tidy
# Quizá te interesara numerar también los párrafos, para que así pudieras localizar las cosas con mayor facilidad
texto_tidy$NumParr <- [1:length(texto_tidy)]
# Quizá te interesara numerar también los párrafos, para que así pudieras localizar las cosas con mayor facilidad
texto_tidy$NumParr <- 1:length(texto_tidy)
# Quizá te interesara numerar también los párrafos, para que así pudieras localizar las cosas con mayor facilidad
texto_tidy$NumParr <- 1:length(texto_tidy$texto)
# Quizá te interesara numerar también los párrafos, para que así pudieras localizar las cosas con mayor facilidad
texto_tidy$NumParr <- seq_along(texto_tidy$texto)
# Ahora lo tienes convertido en una tabla (se llama tibble) de una columna en la que cada párrafo está en una fila
texto_tidy
# Ahora ya podemos empezar a hacerle algunas diabluras…
# Lo primero lo vamos a dividir en oraciones…
texto_oraciones <- unnest_tokens(texto_tidy, texto, output = oración, token = "sentences", to_lower = F)
texto_oraciones
# Ahora las vamos ha identificar con el número de oración
texto_oraciones$NumOracion <- seq_along(texto_oraciones$oración)
texto_oraciones
# Y ahora podemos dividirlo en palabras…
texto_palabras <- texto_tidy %>%
unnest_tokens(texto, palabra)
# Pero el texto que acabamos de ver es poco práctico. Vamos a cargar un artículo de Mariano José Larra
texto_entrada <- read_lines("https://raw.githubusercontent.com/7PartidasDigital/AnalisisTextual/master/textos/larra_caza.txt", skip = 5)
# Ya tienes el texto. Lo que has bajado no tiene el formato adecuado para trabajar dentro del ecosistema tidy.
texto_tidy <- data_frame(parrafo = 1:length(texto_entrada), texto = texto)
# Ya tienes el texto. Lo que has bajado no tiene el formato adecuado para trabajar dentro del ecosistema tidy.
texto_tidy <- data_frame(NumParr = 1:length(texto_entrada), texto = texto_entrada)
# Ahora lo tienes convertido en una tabla (se llama tibble) de una columna en la que cada párrafo está en una fila
texto_tidy
#
texto_tidy %>%
unnest_tokens(palabra, texto)
# Ahora lo podemos dividir en palabras con unnest_tokens que tiene dos argumentos, la columna en la que se va a guardar,
# en este caso palabra y la columna de donde ha de sacar los datos texto.
texto_tidy %>%
unnest_tokens(palabra, texto)
